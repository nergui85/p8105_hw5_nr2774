---
title: "p8105_nr2774"
output: github_document
---

### Problem 1

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(dplyr)
library(rvest)
library(purrr)
library(ggplot2)
library(patchwork)
library(broom.mixed)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

The code chunk below imports the data in individual spreadsheets contained in `./data/zip_data/`. To do this, I create a dataframe that includes the list of all files in that directory and the complete path to each file. As a next step, I `map` over paths and import data using the `read_csv` function. Finally, I `unnest` the result of `map`.

```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest()
```

The result of the previous code chunk isn't tidy -- data are wide rather than long, and some important variables are included as parts of others. The code chunk below tides the data using string manipulations on the file, converting from wide to long, and selecting relevant variables. 

```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Finally, the code chunk below creates a plot showing individual data, faceted by group. 

```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 


### Problem 2

```{r, warning = FALSE, message=FALSE}

df = read_csv(file = "data_hw5/homicide-data.csv") |> 
    janitor::clean_names() |> 
    mutate(city_state = str_c(city, ",", state )) |>
    as.tibble()


```


```{r, warning = FALSE, message=FALSE}
# reported date range 
pull(df,reported_date) |> 
  as.character() %>% 
  range()
#victim's age range
pull(df,victim_age) |> 
    as.character() |> 
    as.integer() |>  
    range(na.rm =TRUE)
```

#### Describe the raw data:
This data set contains `r nrow(df)` homicides records. Reported date range is from 20070101 to 20171231 and victim age range is from 0 to 102 years old. Other information such as victim`s name,race,sex and the location details and disposition state of these homicides were included in this data set. 

In order to check if the number of city_state matches with city.i run the folowoing code.

```{r, warning = FALSE, message=FALSE}
#the number of city_state 
nrow(distinct(df,city_state))
# the number of cities
nrow(distinct(df,city))
#check the dataset and find the city "Tulsa" shows up in two states:
 filter(df,city == "Tulsa") |> 
     distinct(state)
```

Number of city_state were 51 whereas number of state was 50. It means there was an overlap with city names. After filtering, city "Tulsa" was recorded both in Oklahoma and Al. Therefore, Tulsa recorded under Al will be excluded from further data set.  
* Homocide 

```{r, warning = FALSE, message=FALSE}
# Why there is still 51 variables ??
homicide_sum = df |>  
    group_by(city_state) |> 
    summarise(total_homicides = n()) |> 
    filter(city_state != "Tulsa,AL")

```

* Creating homocide summary table 

```{r, warning = FALSE, message=FALSE}
# Summarize within cities to obtain the total number of homicides and the number of unsolved homicides

homicide_sum = df %>% 
  #exclude "Tulsa, AL"
  group_by(city_state) %>% 
  mutate(
    unsolved = ifelse(disposition == "Closed by arrest",0,1)
      ) %>% 
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(unsolved)
  ) %>% 
  arrange(desc(total_homicides)) |> 
    subset(city_state != "Tulsa,AL")
homicide_sum %>% 
  knitr::kable() 
```

* Baltimore analysis

```{r, warning = FALSE, message=FALSE}
# Estimating the proportion of unsolved homicides in Baltimore, MD

baltimore_prop = 
  homicide_sum %>%
  filter(city_state == "Baltimore,MD")
 test_prop = prop.test(baltimore_prop$unsolved_homicides, baltimore_prop$total_homicides)
 
 test_prop %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high)

```

* Iteration for all cities (proportion of unsolved homicides)

```{r, warning = FALSE, message=FALSE}
# Estimating the proportion of unsolved homicides in all cities by applying iteration 
all_cities_proportion = 
  homicide_sum |> 
  mutate(
    tests_prop = purrr::map2(.x = unsolved_homicides, .y = total_homicides, ~prop.test(x = .x, n = .y)),
    tests_tidy = purrr::map(.x = tests_prop, ~broom::tidy(.x))
  ) |>  
  select(-tests_prop) |>  
  unnest(tests_tidy) |>  
  select(city_state, estimate, starts_with("conf"))

```


```{r, warning = FALSE, message=FALSE}
all_cities_proportion |>  
  mutate(city_state = fct_inorder(city_state)) |>  
  ggplot(aes(x = city_state, y = estimate, fill = city_state)) + 
    geom_point() + geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
    theme(axis.text.x = element_text(angle = 90,size = 8)) +
  theme(legend.position = "none") +
  labs(x = "City",y = "Proportion Estimate",title = "Proportion Estimates for All Cities")
      
```


### Problem 3

* Conducting a simulation to explore power in a one-sample t-test.

```{r, warning = FALSE, message=FALSE}
# Construct function for one-sample t-test

sim_mean_ttest = function(true_mean) {
    simulation_data = tibble(x = rnorm(n =30, mean = true_mean, sd = 5)
                             )
test_data = t.test(simulation_data, mu = 0, conf.level = 0.95 ) 
    
  simulation_data %>% 
    summarize(
      estimate = pull(broom::tidy(test_data), estimate),
      p_value = pull(broom::tidy(test_data), p.value)
    )
}

sim_results_df = 
  expand_grid(
    true_mean = c(0:6), 
    iteration = 1:5000
  ) %>%
  mutate(
    estimate_df = map(true_mean, sim_mean_ttest)
  ) %>% 
  unnest(estimate_df)
```


```{r, warning = FALSE, message=FALSE}
# A plot showing the proportion of times the null was rejected

sim_results_df |>
    filter(p_value < 0.05) |> 
    group_by(true_mean) |> 
    summarise(rejected_num = n(), rejected_prop = rejected_num/5000) |> 
    ggplot(aes(x = as.factor(true_mean), y = rejected_prop)) + 
    geom_point() +
    labs(
        title = "The association between effect size and power",
        x = "True value of mean", 
        y = "Proportion of times the null was rejected") 
    
```

#### Describe the association between effect size and power

As the true mean get further away from 0, its gets easier to reject the null value, another words the power increases. The chance that the Null hypothesis will be rejected in increases. Bigger the difference between true mean and the null is, it will get closer to 1, meaning that when effect size increases, the power gets bigger. Therefore, there is a positive correlation between the power of the test and the effect size. 


```{r, warning=FALSE, message=FALSE}
# Average estimate plot 
sim_results_df |>  
  group_by(true_mean) %>% 
  summarise(avg_estimate = mean(estimate)) %>% 
  ggplot(aes(x = true_mean, y = avg_estimate)) +
  geom_point(alpha = 0.5) +
  geom_line(alpha = 0.5) +
  geom_text(aes(label = round(avg_estimate,2)), vjust = -1, size = 3.5) + 
  scale_x_continuous(limits = c(0,6.7), breaks = seq(0,6,1)) +
  scale_y_continuous(limits = c(-0.1,6.7), breaks = seq(0,6,1)) +
  labs(
    title = "Association between average estimates and True mean",
    x = "True mean",
    y = "Average mean estimate"
  ) 
```


```{r, warning=FALSE, message=FALSE}
sim_results_df |> 
    filter(p_value < 0.05) |> 
    group_by(true_mean) |> 
    summarise(rejected_num = n(), rejected_prop = rejected_num/5000) |> 
    ggplot(aes(x = as.factor(true_mean), y = rejected_prop)) + 
    geom_point() +
    labs(
        title = "The association between effect size and power",
        x = "True value of mean", 
        y = "Proportion of times the null was rejected") 

```


```{r}
# The second plot (or overlay on the first) the average estimate of mu only in samples for which the null was rejected on the y axis and the true value of mu on the x axis.

sim_rejected = sim_results_df %>% 
  filter(p_value < 0.05) %>% 
  group_by(true_mean) %>% 
  summarise(avg_estimate = mean(estimate)) 

sim_results_df %>% 
  group_by(true_mean) %>% 
  summarise(avg_estimate = mean(estimate)) %>% 
  ggplot(aes(x = true_mean, y = avg_estimate, color = "Total samples")) +
  geom_point() +
  geom_line() + 
  geom_text(aes(label = round(avg_estimate,2)), vjust = 2, size = 3) +     geom_point(data = sim_rejected, aes(color = "Rejected samples")) +
  geom_line(data = sim_rejected, aes(x = true_mean, y = avg_estimate, color = "Rejected samples")) + 
  geom_text(data = sim_rejected, aes(label = round(avg_estimate,2), color = "Rejected samples"), vjust = -1, size = 3.5) + 
  scale_x_continuous(limits = c(0,6.7), breaks = seq(0,6,1)) +
  scale_y_continuous(limits = c(-0.5,6.7), breaks = seq(0,6,1)) +
  labs(x = "True mean",
       y = "Average mean estimate",
       title = "Association between Average estimates and True mean",
       color = "Type") + scale_color_manual(values = c("Total samples" = 'darkblue', "Rejected samples" = "darkred"))
```

#### Is the sample average of μ̂  across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?

* In terms of total samples, the average estimate means are rougly equal to the true means. On the other hand, this trend was different for the rejected samples. When the true mean was smaller at 0,1,2,3 , the sample average where Null was rejected was different from true mean value. When true mean was is at 4,5,6 average estimate for rejected were roughly the same. In other words, as effect size increases the the probability of rejecting null hyphothesis increases. The larger true mean ( larger/ total sample = close to 1 value) close to value 1 allows us to reject the null hypothesis with less error with more power.    











